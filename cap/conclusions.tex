\chapter{Conclusions}

    \begin{chaptersum}
        Il lavoro presentato mostra come sia possibile automatizzare operazioni ripetitive su dati non omogenei e irregolari, usando soltanto software di analisi geografica \emph{open source}. È stata automatizzata la distinzione tra \emph{ditches} e \emph{compounds}, la derivazione di ulteriori dati geografici relativi alle loro aree, e il calcolo di dati statistici di perimetri, accessi e orientazioni, oltre che la possibilità di osservare e analizzare questi dati usando una apposita interfaccia web che comprende un webGIS.

        L'utilizzo di \emph{software libero} si è rivelato determinante, poiché ha consentito la realizzazione dell'intero sistema a costo zero, e rispetta le regole basilari del metodo scientifico, rendendo trasparente ogni passo del processo che genera nuova conoscenza da dati esistenti. Analogamente, il software presentato si è rivelato adatto a gestire una quantità non esigua di dati, che contiene un grande potenziale di conoscenza, se adeguatamente trattato. I recenti sviluppi nel trattamento di dati geografici in archeologia, geofisica e GIS hanno segnato il movimento verso analisi sempre più impegnative per quantità di dati e ampiezza dei contesti (archeologia del paesaggio) e necessitano quindi di soluzioni tecnologiche accurate ed in grado di organizzare il lavoro su larga scala.

        In questo contesto, a fronte dell'aumento di complessità dei metodi adottati, non sembra esserci uno spostamento verso tecnologie GIS più performanti: questo lavoro suggerisce una possibile soluzione al problema, adottando database spaziali (piuttosto che Shapefile), linguaggi di programmazione open source integrati nei GIS (piuttosto che soluzioni prefabbricate) e sistemi di gestione dati basati su interfacce webGIS piuttosto che desktop.

        Il lavoro presentato mostra alcune criticità minori, di facile risoluzione; tra queste, occorrerebbe risolvere problemi di calcolo nel $4\%$ delle geometrie, scrivere algoritmi più performanti per grandi quantità di dati, permettere l'esportazione dei dati geografici generati (insieme alle statistiche) e consentire il caricamento di dati geografici da fonti ulteriori.
    \end{chaptersum}

    \section{The new approach to GIS data management}
        The current work demonstrates how is possible to programmatically execute repetitive tasks when working with spatial data. This gets a particularly high value since the data are not regularly shaped or distributed. In other words, most of the algorithms presented in this work try to make their way in a set of unordered data, conducting every single case to a known process and extracting new values (geometries) from existing ones. The operation of finding regularities in natural or human phenomenon is the first step toward a better comprehension of the phenomena itself and opens towards the definition of appropriate processing methods.

        With the methods proposed in the previous chapters, a certain number of tasks have been automated: the distinction between \emph{ditches} and \emph{compounds}, the calculation of area, perimeter, access and orientation for each open structure in each settlement, and the automatic generation and plotting of results using webGIS technologies and a standard compliant export system.

        All these results have been obtained strictly using only \emph{free} and \emph{open source} geographic software (FOSS4G) and this leads to a fundamental consideration: when approaching to scientific methods, the open source advantage is evident, because it represents the highest application of the scientific methods when dealing with complex processes. The open source approach places itself in the experimental part of the scientific method, after a hypothesis has been formulated and it must be proved --- or rejected --- using the results of a \emph{reproducible} workflow, which in contemporary science is offered by a transparent, well known, \emph{atomizable} queue of operations. Any other method is a threat to the validity of the scientific process. For this reason, all the code in this work has been released using the GNU GPL v3 free software licence \cite{wiki:gpl}, and portions of the code are presented for reference in app.~\ref{app:code}.

        Another remarkable point concerns how to deal with the increasing complexity of the registered data. With the advancements in archaeology, geophysics, GIS and all the connected disciplines (anthropology, remote sensing, etc.\ ), the comprehension of a phenomenon consists in the analysis of huge datasets more often than in the past, as we are progressively widening our knowledge horizon (moving from archaeology to landscape archeology), basing the new studies on previous published results. This inevitably concerns the study of new ways of managing and analyzing data, and this work moves in this direction.
        
        Generating more data raises the necessity to have better tools, and the consequent availableness of new complex tools generates more data than in the near past. This seems a natural cycle, and could be graphically represented by a step plot. It seems that in geophysics and archaeology we are at the end of a step, since the commonly used systems to manage spatial knowledge (ESRI Shapefiles, desktop systems, etc.\ ) have reached the limit ratio with the quantity of data they can deal with; this is the reason why we need to move to new systems. Most of the tools described in this work are contained in the next step of this imaginary plot: open source technologies, webGIS, spatial databases.

    \section{Further developments}
        While the presented solution is not perfect, it is a good starting point to build new solutions to new problems and enhance our knowledge of nature. Nonetheless, some improvements are easily achievable:

        \begin{description}
            \item[fix glitches in the area drawing algorithm]\hfill\\the errors shown in \fref{sec:derived-areas} are caused algorithm miscalculations when dealing with some compounds' geometries; while the $96\%$ of successfully analyzed compounds seems a good result, this value can be easily risen to $100\%$;
            \item[write more performant code over big amount of data]\hfill\\dealing with huge datasets needs specific expedients in the code; the current code can derive areas from vector data at the speed of 8 geometries per second; any improvement on this process is desirable when the number of geometries will grow in wider contexts;
            \item[export data in a common spatially-enabled format]\hfill\\currently the export function of the software only supports CSV files (\fref{sec:webgis}), since the exportation is limited to the numerical statistic values; with small efforts, the geographical data could be exported as spatial files (like geoJSON or ESRI Shapefiles);
            \item[import data from alternative sources]\hfill\\at the current status, only compressed ESRI Shapefiles can be imported; enabling various data sources will offer more flexibility and ease the work of the analyst.
        \end{description}
